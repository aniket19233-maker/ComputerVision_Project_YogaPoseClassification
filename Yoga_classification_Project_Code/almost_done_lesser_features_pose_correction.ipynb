{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "O1fVsWOz5pXK",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /home/hardeekh/.local/lib/python3.8/site-packages (0.8.9.1)\n",
      "Requirement already satisfied: numpy in /home/hardeekh/.local/lib/python3.8/site-packages (from mediapipe) (1.22.2)\n",
      "Requirement already satisfied: absl-py in /home/hardeekh/.local/lib/python3.8/site-packages (from mediapipe) (0.13.0)\n",
      "Requirement already satisfied: matplotlib in /home/hardeekh/.local/lib/python3.8/site-packages (from mediapipe) (3.3.4)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/lib/python3/dist-packages (from mediapipe) (19.3.0)\n",
      "Requirement already satisfied: opencv-contrib-python in /home/hardeekh/.local/lib/python3.8/site-packages (from mediapipe) (4.5.5.62)\n",
      "Requirement already satisfied: protobuf>=3.11.4 in /home/hardeekh/.local/lib/python3.8/site-packages (from mediapipe) (3.17.3)\n",
      "Requirement already satisfied: six in /home/hardeekh/.local/lib/python3.8/site-packages (from absl-py->mediapipe) (1.15.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib->mediapipe) (7.0.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/lib/python3/dist-packages (from matplotlib->mediapipe) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/hardeekh/.local/lib/python3.8/site-packages (from matplotlib->mediapipe) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/hardeekh/.local/lib/python3.8/site-packages (from matplotlib->mediapipe) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hardeekh/.local/lib/python3.8/site-packages (from matplotlib->mediapipe) (0.10.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "GqRsPYZb59g0"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from numpy.linalg import norm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "mHB9ucoB5pXM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_metrics(y_true, y_pred, name):\n",
    "    print(25*\"##\")\n",
    "    print(name)\n",
    "    print(\"Accuracy:\",accuracy_score(y_test_list,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_test_list,y_pred, average=\"weighted\"))\n",
    "    print(25*\"##\")\n",
    "    \n",
    "def run_models(x_train, x_test, y_train, y_test, custom):\n",
    "    \n",
    "    model_dict = {}\n",
    "    \n",
    "    ## logistic regression\n",
    "    model_logistic = LogisticRegression()\n",
    "    model_logistic.fit(x_train,y_train)\n",
    "    y_pred = model_logistic.predict(x_test)\n",
    "    print(25*\"##\")\n",
    "    print(\"LOGISTIC REGRESSION\")\n",
    "    print(\"Testing Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
    "    #print(classification_report(y_test,y_pred))\n",
    "    #print(25*\"**\")\n",
    "    y_pred = model_logistic.predict(x_train)\n",
    "    print(25*\"--\")\n",
    "    print(\"Training Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
    "    ##print(classification_report(y_train,y_pred))\n",
    "    \n",
    "    ## SGD classifier\n",
    "    model_SGD = SGDClassifier()\n",
    "    model_SGD.fit(x_train,y_train)\n",
    "    y_pred = model_SGD.predict(x_test)\n",
    "    print(25*\"##\")\n",
    "    print(\"SGD CLASSIFIER\")\n",
    "    print(\"Testing Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
    "    #print(classification_report(y_test,y_pred))\n",
    "    #print(25*\"**\")\n",
    "    y_pred = model_SGD.predict(x_train)\n",
    "    print(25*\"--\")\n",
    "    print(\"Training Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
    "    ##print(classification_report(y_train,y_pred))\n",
    "    \n",
    "    ## Naive Bayes\n",
    "    model_NB = GaussianNB()\n",
    "    model_NB.fit(x_train,y_train)\n",
    "    y_pred = model_NB.predict(x_test)\n",
    "    print(25*\"##\")\n",
    "    print(\"GAUSSIAN NAIVE BAYES\")\n",
    "    print(\"Testing Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
    "    #print(classification_report(y_test,y_pred))\n",
    "    #print(25*\"**\")\n",
    "    y_pred = model_NB.predict(x_train)\n",
    "    print(25*\"--\")\n",
    "    print(\"Training Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
    "    ##print(classification_report(y_train,y_pred))\n",
    "    \n",
    "    ## Random Forest\n",
    "    model_RF = RandomForestClassifier()\n",
    "    model_RF.fit(x_train,y_train)\n",
    "    y_pred = model_RF.predict(x_test)\n",
    "    print(25*\"##\")\n",
    "    print(\"RANDOM FOREST\")\n",
    "    print(\"Testing Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
    "    #print(classification_report(y_test,y_pred))\n",
    "    #print(25*\"**\")\n",
    "    y_pred = model_RF.predict(x_train)\n",
    "    print(25*\"--\")\n",
    "    print(\"Training Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
    "    ##print(classification_report(y_train,y_pred))\n",
    "    \n",
    "    ## XGBoost\n",
    "    model_xgboost = XGBClassifier()\n",
    "    model_xgboost.fit(x_train,y_train)\n",
    "    y_pred = model_xgboost.predict(x_test)\n",
    "    print(25*\"##\")\n",
    "    print(\"XGBOOST\")\n",
    "    print(\"Testing Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
    "    #print(classification_report(y_test,y_pred))\n",
    "    #print(25*\"**\")\n",
    "    y_pred = model_xgboost.predict(x_train)\n",
    "    print(25*\"--\")\n",
    "    print(\"Training Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
    "    ##print(classification_report(y_train,y_pred))\n",
    "    \n",
    "    ## AdaBoost\n",
    "    model_adaboost = AdaBoostClassifier()\n",
    "    model_adaboost.fit(x_train,y_train)\n",
    "    y_pred = model_adaboost.predict(x_test)\n",
    "    print(25*\"##\")\n",
    "    print(\"ADABOOST\")\n",
    "    print(\"Testing Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
    "    #print(classification_report(y_test,y_pred))\n",
    "    #print(25*\"**\")\n",
    "    y_pred = model_adaboost.predict(x_train)\n",
    "    print(25*\"--\")\n",
    "    print(\"Training Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
    "    ##print(classification_report(y_train,y_pred))\n",
    "    \n",
    "    ## KNN\n",
    "    model_knn = KNeighborsClassifier()\n",
    "    model_knn.fit(x_train,y_train)\n",
    "    y_pred = model_knn.predict(x_test)\n",
    "    print(25*\"##\")\n",
    "    print(\"KNN CLASSIFIER\")\n",
    "    print(\"Testing Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
    "    #print(classification_report(y_test,y_pred))\n",
    "    #print(25*\"**\")\n",
    "    y_pred = model_knn.predict(x_train)\n",
    "    print(25*\"--\")\n",
    "    print(\"Training Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
    "    ##print(classification_report(y_train,y_pred))\n",
    "    \n",
    "    ## SVM\n",
    "    model_SVM = SVC()\n",
    "    model_SVM.fit(x_train,y_train)\n",
    "    y_pred = model_SVM.predict(x_test)\n",
    "    print(25*\"##\")\n",
    "    print(\"SVM\")\n",
    "    print(\"Testing Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
    "    #print(classification_report(y_test,y_pred))\n",
    "    #print(25*\"**\")\n",
    "    y_pred = model_SVM.predict(x_train)\n",
    "    print(25*\"--\")\n",
    "    print(\"Training Data:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
    "    ##print(classification_report(y_train,y_pred))\n",
    "\n",
    "    model_dict[\"logistic\"] = model_logistic\n",
    "    model_dict[\"SGD\"] = model_SGD\n",
    "    model_dict[\"RF\"] = model_RF\n",
    "    model_dict[\"KNN\"] = model_knn\n",
    "    model_dict[\"SVM\"] = model_SVM\n",
    "    model_dict[\"XGB\"] = model_xgboost\n",
    "    model_dict[\"ADA\"] = model_adaboost\n",
    "    model_dict[\"NB\"] = model_NB\n",
    "    \n",
    "    name = \"raw_feature_model_dict\"\n",
    "    if custom:\n",
    "        name = \"custom_feature_model_dict.pkl\"\n",
    "    \n",
    "    with open(name,\"wb\") as handle:\n",
    "        pickle.dump(model_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "jzK0_XLwAhUi"
   },
   "outputs": [],
   "source": [
    "def get_cosine_similarity_results(x_train, x_test, y_train, y_test, features):\n",
    "    #print(x_train[:10])\n",
    "    #print(y_train[:10])\n",
    "    y_pred = []\n",
    "    \n",
    "    dataset = pd.concat([x_train, y_train], axis=1)\n",
    "    dict1 = {} #pose -> 132 size array with mean keypoints\n",
    "\n",
    "    for pose in range(0,81):\n",
    "      temp = dataset.loc[dataset['label'] == pose].mean()\n",
    "      temp = temp.drop(labels=['label'])\n",
    "      dict1[pose] = temp\n",
    "    #print(dict1)\n",
    "    \n",
    "    with open(\"cosine_dict.pkl\",\"wb\") as handle:\n",
    "        pickle.dump(dict1,handle)\n",
    "        \n",
    "    feature_wise_cosine_similarity = {} ## i'th data_point -> {}\n",
    "    \n",
    "    feature_wise_cosine_similarity[\"data_point\"] = []\n",
    "    feature_wise_cosine_similarity[\"actual_pose\"] = []\n",
    "    feature_wise_cosine_similarity[\"predicted_pose\"] = []\n",
    "    \n",
    "    for i in tqdm(range(x_test.shape[0])):\n",
    "\n",
    "        curr_row = [x_test.iloc[i]]\n",
    "\n",
    "        curr_y = -1\n",
    "        curr_sim_score = -float(\"inf\")\n",
    "\n",
    "        for pose in range(0,81):\n",
    "          row = dict1[pose]\n",
    "          #print(row)\n",
    "          #print(\"----\")\n",
    "          #print(curr_row)\n",
    "          row = row.to_numpy()\n",
    "          curr_row = np.array(curr_row)\n",
    "          sim_score = cosine_similarity(row.reshape(1, -1),curr_row.reshape(1,-1))\n",
    "\n",
    "          if sim_score > curr_sim_score:\n",
    "            curr_sim_score = sim_score\n",
    "            curr_y = pose\n",
    "\n",
    "        \"\"\"for j in range(x_train.shape[0]):\n",
    "\n",
    "            row = [x_train.iloc[j,:]]\n",
    "\n",
    "            sim_score = cosine_similarity(row,curr_row)\n",
    "\n",
    "            if sim_score > curr_sim_score:\n",
    "                curr_sim_score = sim_score\n",
    "                curr_y = y_train.iloc[j]\"\"\"\n",
    "        \n",
    "        feature_wise_cosine_similarity[\"data_point\"].append(i)\n",
    "        feature_wise_cosine_similarity[\"predicted_pose\"].append(le.inverse_transform([curr_y])[0])\n",
    "        feature_wise_cosine_similarity[\"actual_pose\"].append(le.inverse_transform([list(y_test)[i]])[0])\n",
    "        \n",
    "        row = dict1[curr_y]\n",
    "        for j in range(len(features)-1):\n",
    "            \n",
    "            if features[j] not in feature_wise_cosine_similarity:\n",
    "                feature_wise_cosine_similarity[features[j]] = []\n",
    "                \n",
    "            n1 = row[j]\n",
    "            n2 = curr_row[0][j]\n",
    "            sim = 1 - abs(n1 - n2) / (n1 + n2)\n",
    "            feature_wise_cosine_similarity[features[j]] = sim\n",
    "        \n",
    "        y_pred.append(curr_y)\n",
    "\n",
    "    y_test_list = list(y_test)\n",
    "    \n",
    "    print(25*\"##\")\n",
    "    print(\"COSINE SIMILARITY RESULTS:\")\n",
    "    print(\"Accuracy:\",accuracy_score(y_test_list,y_pred))\n",
    "    print(\"F1 Score:\",f1_score(y_test_list,y_pred, average=\"weighted\"))\n",
    "    \n",
    "    correction_df = pd.DataFrame(feature_wise_cosine_similarity)\n",
    "    \n",
    "    if len(features) >= 100:\n",
    "        correction_df.to_csv(\"correction_dataframe_raw.csv\")\n",
    "    else:\n",
    "        correction_df.to_csv(\"correction_dataframe_custom.csv\")\n",
    "    \n",
    "    print(25*\"##\")\n",
    "    \n",
    "    \n",
    "def cosine_eval(x_test, y_test, y_pred, features):\n",
    "    #print(x_train[:10])\n",
    "    #print(y_train[:10])\n",
    "    \n",
    "    with open(\"cosine_dict.pkl\",\"rb\") as handle:\n",
    "        dict1 = pickle.load(handle)\n",
    "        \n",
    "    feature_wise_cosine_similarity = {} ## i'th data_point -> {}\n",
    "    \n",
    "    feature_wise_cosine_similarity[\"data_point\"] = []\n",
    "    feature_wise_cosine_similarity[\"actual_pose\"] = []\n",
    "    feature_wise_cosine_similarity[\"predicted_pose\"] = []\n",
    "    \n",
    "    for i in tqdm(range(x_test.shape[0])):\n",
    "\n",
    "        curr_row = [x_test.iloc[i]]\n",
    "      \n",
    "        feature_wise_cosine_similarity[\"data_point\"].append(i)\n",
    "        \n",
    "        feature_wise_cosine_similarity[\"predicted_pose\"].append(le.inverse_transform([y_pred[i]])[0])\n",
    "        feature_wise_cosine_similarity[\"actual_pose\"].append(le.inverse_transform([list(y_test)[i]])[0])\n",
    "        \n",
    "        row = dict1[y_pred[i]]\n",
    "        for j in range(len(features)-1):\n",
    "            \n",
    "            if features[j] not in feature_wise_cosine_similarity:\n",
    "                feature_wise_cosine_similarity[features[j]] = []\n",
    "                \n",
    "            n1 = row[j]\n",
    "            n2 = curr_row[0][j]\n",
    "            sim = 1 - abs(n1 - n2) / (n1 + n2)\n",
    "            feature_wise_cosine_similarity[features[j]] = sim\n",
    "        \n",
    "\n",
    "    y_test_list = list(y_test)\n",
    "    \n",
    "    print(\"dataframe saved with eval results!!\")\n",
    "    \n",
    "    correction_df = pd.DataFrame(feature_wise_cosine_similarity)\n",
    "    \n",
    "    if len(features) >= 100:\n",
    "        correction_df.to_csv(\"correction_dataframe_raw.csv\")\n",
    "    else:\n",
    "        correction_df.to_csv(\"correction_dataframe_custom.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "r8yDasmZ5pXO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_raw_data_results(df):\n",
    "    \n",
    "    ## converting output to numeric values\n",
    "    le.fit(df[\"Pose\"])\n",
    "    df[\"label\"] = le.transform(df[\"Pose\"])\n",
    "    \n",
    "    ## dropping Pose column\n",
    "    df.drop(columns=[\"Pose\",\"ImgNum\"],inplace=True)\n",
    "    \n",
    "    ## feature designing for train and test\n",
    "    df #= get_designed_data_df(df)\n",
    "    \n",
    "    x = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,stratify=y,test_size=0.25, random_state=0)\n",
    "\n",
    "    \"\"\"dataset = pd.concat([x_train, y_train], axis=1)\n",
    "    dict1 = {} #pose -> 132 size array with mean keypoints\n",
    "\n",
    "    for pose in range(0,81):\n",
    "      temp = dataset.loc[dataset['label'] == pose].mean()\n",
    "      dict1[pose] = temp\n",
    "    print(dict1)\"\"\"\n",
    "\n",
    "    \n",
    "    ## running different models on this\n",
    "    #model_dict = run_models(x_train, x_test, y_train, y_test,False)\n",
    "    \n",
    "    #return model_dict\n",
    "    ## get cosine similarity results\n",
    "    get_cosine_similarity_results(x_train, x_test, y_train, y_test, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zhUbXFp5xX3",
    "outputId": "0cb07f73-273b-422a-8b5d-cf53b7f116fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3879/3879 [01:02<00:00, 62.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "COSINE SIMILARITY RESULTS:\n",
      "Accuracy: 0.277133281773653\n",
      "F1 Score: 0.2944410321729524\n",
      "      data_point                                        actual_pose  \\\n",
      "0              0                            Sitting pose 1 (normal)   \n",
      "1              1  Extended_Revolved_Side_Angle_Pose_or_Utthita_P...   \n",
      "2              2                     Scorpion_pose_or_vrischikasana   \n",
      "3              3  Pose_Dedicated_to_the_Sage_Koundinya_or_Eka_Pa...   \n",
      "4              4                           Supta_Virasana_Vajrasana   \n",
      "...          ...                                                ...   \n",
      "3874        3874  Half_Lord_of_the_Fishes_Pose_or_Ardha_Matsyend...   \n",
      "3875        3875  Standing_big_toe_hold_pose_or_Utthita_Padangus...   \n",
      "3876        3876                                      Cockerel_Pose   \n",
      "3877        3877                             Plow_Pose_or_Halasana_   \n",
      "3878        3878                             Supta_Baddha_Konasana_   \n",
      "\n",
      "                                         predicted_pose    NOSE-x    NOSE-y  \\\n",
      "0                               Sitting pose 1 (normal)  0.792834  0.911612   \n",
      "1                             Eagle_Pose_or_Garudasana_  0.792834  0.911612   \n",
      "2             Legs-Up-the-Wall_Pose_or_Viparita_Karani_  0.792834  0.911612   \n",
      "3                           Locust_Pose_or_Salabhasana_  0.792834  0.911612   \n",
      "4                              Corpse_Pose_or_Savasana_  0.792834  0.911612   \n",
      "...                                                 ...       ...       ...   \n",
      "3874               Bound_Angle_Pose_or_Baddha_Konasana_  0.792834  0.911612   \n",
      "3875                    Low_Lunge_pose_or_Anjaneyasana_  0.792834  0.911612   \n",
      "3876                                      Cockerel_Pose  0.792834  0.911612   \n",
      "3877  Pose_Dedicated_to_the_Sage_Koundinya_or_Eka_Pa...  0.792834  0.911612   \n",
      "3878                           Supta_Virasana_Vajrasana  0.792834  0.911612   \n",
      "\n",
      "        NOSE-z    NOSE-v  LEFT_EYE_INNER-x  LEFT_EYE_INNER-y  \\\n",
      "0     1.050613  0.948024          0.756111          0.905048   \n",
      "1     1.050613  0.948024          0.756111          0.905048   \n",
      "2     1.050613  0.948024          0.756111          0.905048   \n",
      "3     1.050613  0.948024          0.756111          0.905048   \n",
      "4     1.050613  0.948024          0.756111          0.905048   \n",
      "...        ...       ...               ...               ...   \n",
      "3874  1.050613  0.948024          0.756111          0.905048   \n",
      "3875  1.050613  0.948024          0.756111          0.905048   \n",
      "3876  1.050613  0.948024          0.756111          0.905048   \n",
      "3877  1.050613  0.948024          0.756111          0.905048   \n",
      "3878  1.050613  0.948024          0.756111          0.905048   \n",
      "\n",
      "      LEFT_EYE_INNER-z  ...  RIGHT_HEEL-z  RIGHT_HEEL-v  LEFT_FOOT_INDEX-x  \\\n",
      "0             1.017481  ...      0.736023      0.577409           0.734643   \n",
      "1             1.017481  ...      0.736023      0.577409           0.734643   \n",
      "2             1.017481  ...      0.736023      0.577409           0.734643   \n",
      "3             1.017481  ...      0.736023      0.577409           0.734643   \n",
      "4             1.017481  ...      0.736023      0.577409           0.734643   \n",
      "...                ...  ...           ...           ...                ...   \n",
      "3874          1.017481  ...      0.736023      0.577409           0.734643   \n",
      "3875          1.017481  ...      0.736023      0.577409           0.734643   \n",
      "3876          1.017481  ...      0.736023      0.577409           0.734643   \n",
      "3877          1.017481  ...      0.736023      0.577409           0.734643   \n",
      "3878          1.017481  ...      0.736023      0.577409           0.734643   \n",
      "\n",
      "      LEFT_FOOT_INDEX-y  LEFT_FOOT_INDEX-z  LEFT_FOOT_INDEX-v  \\\n",
      "0              0.997233           0.835635            0.49226   \n",
      "1              0.997233           0.835635            0.49226   \n",
      "2              0.997233           0.835635            0.49226   \n",
      "3              0.997233           0.835635            0.49226   \n",
      "4              0.997233           0.835635            0.49226   \n",
      "...                 ...                ...                ...   \n",
      "3874           0.997233           0.835635            0.49226   \n",
      "3875           0.997233           0.835635            0.49226   \n",
      "3876           0.997233           0.835635            0.49226   \n",
      "3877           0.997233           0.835635            0.49226   \n",
      "3878           0.997233           0.835635            0.49226   \n",
      "\n",
      "      RIGHT_FOOT_INDEX-x  RIGHT_FOOT_INDEX-y  RIGHT_FOOT_INDEX-z  \\\n",
      "0               0.711802            0.862152            0.787422   \n",
      "1               0.711802            0.862152            0.787422   \n",
      "2               0.711802            0.862152            0.787422   \n",
      "3               0.711802            0.862152            0.787422   \n",
      "4               0.711802            0.862152            0.787422   \n",
      "...                  ...                 ...                 ...   \n",
      "3874            0.711802            0.862152            0.787422   \n",
      "3875            0.711802            0.862152            0.787422   \n",
      "3876            0.711802            0.862152            0.787422   \n",
      "3877            0.711802            0.862152            0.787422   \n",
      "3878            0.711802            0.862152            0.787422   \n",
      "\n",
      "      RIGHT_FOOT_INDEX-v  \n",
      "0               0.318286  \n",
      "1               0.318286  \n",
      "2               0.318286  \n",
      "3               0.318286  \n",
      "4               0.318286  \n",
      "...                  ...  \n",
      "3874            0.318286  \n",
      "3875            0.318286  \n",
      "3876            0.318286  \n",
      "3877            0.318286  \n",
      "3878            0.318286  \n",
      "\n",
      "[3879 rows x 135 columns]\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "df = pd.read_csv(\"trainSet_yoga82.csv\")\n",
    "get_raw_data_results(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "jh55FFVF5pXO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_angle(p1, p2, p3):\n",
    "    \n",
    "    \"\"\"\n",
    "        returns angle between p1,p2,p3 with p2 as the pivot\n",
    "    \"\"\"\n",
    "    \n",
    "    v1 = p1-p2\n",
    "    v2 = p3-p2\n",
    "    \n",
    "    num = np.inner(v1,v2)\n",
    "    den = (np.sqrt((v1**2).sum())) * (np.sqrt((v2**2).sum()))\n",
    "    \n",
    "    return np.degrees(np.arccos(num/den))\n",
    "\n",
    "def get_designed_data_df(df):\n",
    "    \n",
    "    d = {}\n",
    "\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "\n",
    "        j=0\n",
    "        while j<df.shape[1]-1:\n",
    "\n",
    "            col_name = df.columns[j].split(\"-\")[0]\n",
    "\n",
    "            data_point = np.array([df.iloc[i,j], df.iloc[i,j+1], df.iloc[i,j+2]])\n",
    "            col_name = col_name.lower()\n",
    "\n",
    "            if col_name not in d:\n",
    "                d[col_name] = []\n",
    "\n",
    "            d[col_name].append(data_point)\n",
    "            j+=4\n",
    "\n",
    "        if \"label\" not in d:\n",
    "            d[\"label\"] = []\n",
    "        d[\"label\"].append(df.iloc[i,j]) \n",
    "        \n",
    "    return get_custom_features(pd.DataFrame(d))\n",
    "    \n",
    "def get_custom_features(df):\n",
    "    \n",
    "    ## add features\n",
    "\n",
    "    d = {}\n",
    "\n",
    "    ## angles\n",
    "    d[\"max_hand_angle\"] = []\n",
    "    d[\"min_hand_angle\"] = []\n",
    "    d[\"max_foot_angle\"] = []\n",
    "    d[\"min_foot_angle\"] = []\n",
    "\n",
    "    d[\"max_elbow_to_knee_angle\"] = []\n",
    "    d[\"min_elbow_to_knee_angle\"] = []\n",
    "\n",
    "    d[\"knee_to_knee_angle\"] = []\n",
    "    d[\"elbow_to_elbow_angle\"] = []\n",
    "\n",
    "    #d[\"heel_at_nose_angle\"] = []\n",
    "    #d[\"hand_at_nose_angle\"] = []\n",
    "    d[\"max_nose_to_heel_angle\"] = []\n",
    "\n",
    "    ## distances\n",
    "    d[\"feet_to_shoulder_ratio\"] = []\n",
    "    d[\"hand_to_shoulder_ratio\"] = []\n",
    "\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "\n",
    "        ## hand angles\n",
    "        angle1 = get_angle(df[\"left_wrist\"][i], df[\"left_elbow\"][i], df[\"left_shoulder\"][i])\n",
    "        angle2 = get_angle(df[\"right_wrist\"][i], df[\"right_elbow\"][i], df[\"right_shoulder\"][i])\n",
    "\n",
    "        d[\"max_hand_angle\"].append(max(angle1,angle2))\n",
    "        d[\"min_hand_angle\"].append(min(angle1,angle2))\n",
    "\n",
    "        ## leg angles\n",
    "        angle1 = get_angle(df[\"left_ankle\"][i], df[\"left_knee\"][i], df[\"left_hip\"][i])\n",
    "        angle2 = get_angle(df[\"right_ankle\"][i], df[\"right_knee\"][i], df[\"right_hip\"][i])\n",
    "\n",
    "        d[\"max_foot_angle\"].append(max(angle1,angle2))\n",
    "        d[\"min_foot_angle\"].append(min(angle1,angle2))\n",
    "\n",
    "        ## hand to leg angles\n",
    "        # wrist to leg angle instead of elbow to knee\n",
    "        angle1 = get_angle(df[\"left_elbow\"][i], df[\"left_hip\"][i], df[\"left_knee\"][i])\n",
    "        angle2 = get_angle(df[\"right_elbow\"][i], df[\"right_hip\"][i], df[\"right_knee\"][i])\n",
    "        #angle1 = get_angle(df[\"left_wrist\"][i], df[\"left_hip\"][i], df[\"left_ankle\"][i])\n",
    "        #angle2 = get_angle(df[\"right_wrist\"][i], df[\"right_hip\"][i], df[\"right_ankle\"][i])\n",
    "\n",
    "        d[\"max_elbow_to_knee_angle\"].append(max(angle1,angle2))\n",
    "        d[\"min_elbow_to_knee_angle\"].append(min(angle1,angle2))\n",
    "\n",
    "        ## elbow to elbow\n",
    "        # try elbow-centroid-elbow instead\n",
    "        mid_point = (df[\"left_shoulder\"][i] + df[\"right_shoulder\"][i]) / 2\n",
    "        angle = get_angle(df[\"left_elbow\"][i], mid_point, df[\"right_elbow\"][i])\n",
    "\n",
    "        #centroid = np.mean(df.iloc[i])\n",
    "        #angle = get_angle(df[\"left_elbow\"][i], centroid, df[\"right_elbow\"][i])\n",
    "\n",
    "        d[\"elbow_to_elbow_angle\"].append(angle)\n",
    "\n",
    "        ## knee to knee\n",
    "        mid_point = (df[\"left_hip\"][i] + df[\"right_hip\"][i]) / 2\n",
    "        angle = get_angle(df[\"left_knee\"][i], mid_point, df[\"right_knee\"][i])\n",
    "\n",
    "        #angle = get_angle(df[\"left_knee\"][i], centroid, df[\"right_knee\"][i])\n",
    "        d[\"knee_to_knee_angle\"].append(angle)\n",
    "\n",
    "        ## heels at nose and hand at nose angle\n",
    "        # heel-waist-heel instead of heel-nose-heel\n",
    "        #angle1 = get_angle(df[\"left_wrist\"][i], df[\"nose\"][i], df[\"right_wrist\"][i])\n",
    "        #angle2 = get_angle(df[\"left_heel\"][i], df[\"nose\"][i], df[\"right_heel\"][i])\n",
    "        #mid_point = (df[\"left_hip\"][i] + df[\"right_hip\"][i]) / 2\n",
    "        #angle2 = get_angle(df[\"left_heel\"][i], mid_point, df[\"right_heel\"][i])\n",
    "\n",
    "        #d[\"hand_at_nose_angle\"].append(angle1)\n",
    "        #d[\"heel_at_nose_angle\"].append(angle2)\n",
    "\n",
    "        # nose-waist-heel\n",
    "        midpoint = (df[\"left_hip\"][i] + df[\"right_hip\"][i]) / 2\n",
    "        angle1 = get_angle(df[\"nose\"][i], midpoint, df[\"right_heel\"][i])\n",
    "        angle2 = get_angle(df[\"nose\"][i], midpoint, df[\"left_heel\"][i])\n",
    "\n",
    "        d[\"max_nose_to_heel_angle\"].append(max(angle1,angle2))\n",
    "\n",
    "        ############################################\n",
    "        \n",
    "        ## centroid distance \n",
    "        centroid = np.mean(df.iloc[i])\n",
    "        incl_cols = [\"left_wrist\", \"left_elbow\", \"left_shoulder\", \"left_eye\", \"left_hip\", \"left_knee\", \"left_ankle\",\n",
    "                     \"right_wrist\", \"right_elbow\", \"right_shoulder\", \"right_eye\", \"right_hip\", \"right_knee\", \"right_ankle\"]\n",
    "        for col in df.columns:\n",
    "  \n",
    "            if col in incl_cols:\n",
    "                \n",
    "                name = col+\"_centroid_distance\"\n",
    "                if name not in d:\n",
    "                    d[name] = []\n",
    "\n",
    "                #print(col, df[col][i])\n",
    "                d[name].append(np.sqrt(np.sum(np.square(df[col][i]-centroid))))\n",
    "        \n",
    "        ## feet and hand to shoulder ratio\n",
    "\n",
    "        d_shoulder = np.sqrt(np.sum(np.square(df[\"left_shoulder\"][i] - df[\"right_shoulder\"][i])))\n",
    "        d_hand = np.sqrt(np.sum(np.square(df[\"left_wrist\"][i] - df[\"right_wrist\"][i])))\n",
    "        d_feet = np.sqrt(np.sum(np.square(df[\"left_foot_index\"][i] - df[\"right_foot_index\"][i])))\n",
    "\n",
    "        d[\"feet_to_shoulder_ratio\"].append(d_feet/d_shoulder)\n",
    "        d[\"hand_to_shoulder_ratio\"].append(d_hand/d_shoulder)\n",
    "\n",
    "    ## output\n",
    "    if \"label\" in df:\n",
    "        d[\"label\"] = df[\"label\"]\n",
    "    \n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "PjDidNPm5pXP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_designed_data_results(df):\n",
    "    \n",
    "    ## converting output to numeric values\n",
    "    le.fit(df[\"Pose\"])\n",
    "    df[\"label\"] = le.transform(df[\"Pose\"])\n",
    "    \n",
    "    ## dropping Pose column\n",
    "    df.drop(columns=[\"Pose\",\"ImgNum\"],inplace=True)\n",
    "    \n",
    "   \n",
    "    ## feature designing for train and test\n",
    "    df = get_designed_data_df(df)\n",
    "    \n",
    "    x = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,stratify=y,test_size=0.25, random_state=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model_dict = None\n",
    "    ## run models\n",
    "    model_dict = run_models(x_train, x_test, y_train, y_test,True)\n",
    "    \n",
    "    ## cosine similarity\n",
    "    get_cosine_similarity_results(x_train, x_test, y_train, y_test, df.columns)\n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "ORkXX_i15pXP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_image(img_path, model, le, design_features):\n",
    "    \n",
    "    ## get image keypoints\n",
    "    mp_pose = mp.solutions.pose\n",
    "    mp_drawing = mp.solutions.drawing_utils \n",
    "    mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "    \n",
    "    with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:\n",
    "        \n",
    "        landmark_names = []\n",
    "        \n",
    "        for name in mp_pose.PoseLandmark:\n",
    "            name = str(name).lower().split(\".\")[-1]\n",
    "            landmark_names.append(name)\n",
    "            \n",
    "        img = cv2.imread(img_path)\n",
    "        results = pose.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "      \n",
    "        row = {}\n",
    "        for i in range(0,33):\n",
    "            coord = results.pose_landmarks.landmark[i]\n",
    "  \n",
    "            if landmark_names[i] not in row:\n",
    "                row[landmark_names[i]] = []\n",
    "            \n",
    "            row[landmark_names[i]].append(np.array([coord.x, coord.y, coord.z]))\n",
    "        \n",
    "    df = pd.DataFrame(row)\n",
    "    \n",
    "    if design_features:\n",
    "        \n",
    "        df = get_custom_features(df)\n",
    "        \n",
    "    \"\"\"\n",
    "    CAN VISUALIZE AND ADD NOTION OF NOISE CLASS HERE\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    y_proba = model.predict_proba(df)\n",
    "    name = le.inverse_transform(model.predict(df))\n",
    "    print(\"predicted_pose:\",name)\n",
    "    print(\"getting cosine similarity results...\")\n",
    "    y_test = [\"\" for i in range(df.shape[0])]\n",
    "    y_pred = model.predict(df)\n",
    "    \n",
    " #   print(y_test)\n",
    " #   print(y_pred[0])\n",
    " \n",
    "    cosine_eval(df, [1 for i in range(df.shape[0])], y_pred, df.columns)\n",
    "       # x_train, df, y_train, [\"\" for i in range(df.shape[0])], model.predict(df), df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15516/15516 [00:39<00:00, 389.20it/s]\n",
      "100%|██████████| 15516/15516 [00:16<00:00, 943.00it/s] \n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "LOGISTIC REGRESSION\n",
      "Testing Data:\n",
      "Accuracy: 0.5158546017014695\n",
      "F1 Score: 0.4909363250850151\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 0.5305491105955144\n",
      "F1 Score: 0.5096246665820652\n",
      "##################################################\n",
      "SGD CLASSIFIER\n",
      "Testing Data:\n",
      "Accuracy: 0.2853828306264501\n",
      "F1 Score: 0.23108435941885627\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 0.2950073042880467\n",
      "F1 Score: 0.23885190322656408\n",
      "##################################################\n",
      "GAUSSIAN NAIVE BAYES\n",
      "Testing Data:\n",
      "Accuracy: 0.8819283320443413\n",
      "F1 Score: 0.8829747044804347\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 0.8950760505284867\n",
      "F1 Score: 0.895597338054792\n",
      "##################################################\n",
      "RANDOM FOREST\n",
      "Testing Data:\n",
      "Accuracy: 0.922918277906677\n",
      "F1 Score: 0.9224669358946616\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 1.0\n",
      "F1 Score: 1.0\n",
      "##################################################\n",
      "XGBOOST\n",
      "Testing Data:\n",
      "Accuracy: 0.8824439288476411\n",
      "F1 Score: 0.8815296312947964\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 1.0\n",
      "F1 Score: 1.0\n",
      "##################################################\n",
      "ADABOOST\n",
      "Testing Data:\n",
      "Accuracy: 0.05387986594483114\n",
      "F1 Score: 0.015653923115074894\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 0.05534072355418063\n",
      "F1 Score: 0.015036367729406952\n",
      "##################################################\n",
      "KNN CLASSIFIER\n",
      "Testing Data:\n",
      "Accuracy: 0.5756638308842486\n",
      "F1 Score: 0.5659607991983681\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 0.694079230042107\n",
      "F1 Score: 0.6876871535520062\n",
      "##################################################\n",
      "SVM\n",
      "Testing Data:\n",
      "Accuracy: 0.5186903841196184\n",
      "F1 Score: 0.48651127561535146\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 0.5361347426312624\n",
      "F1 Score: 0.5043706655232948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3879/3879 [01:13<00:00, 52.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "COSINE SIMILARITY RESULTS:\n",
      "Accuracy: 0.3663315287445218\n",
      "F1 Score: 0.3690172163524\n",
      "##################################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logistic': LogisticRegression(),\n",
       " 'SGD': SGDClassifier(),\n",
       " 'RF': RandomForestClassifier(),\n",
       " 'KNN': KNeighborsClassifier(),\n",
       " 'SVM': SVC(),\n",
       " 'XGB': XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "               colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "               early_stopping_rounds=None, enable_categorical=False,\n",
       "               eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "               importance_type=None, interaction_constraints='',\n",
       "               learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "               max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "               missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "               n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n",
       "               predictor='auto', random_state=0, reg_alpha=0, ...),\n",
       " 'ADA': AdaBoostClassifier(),\n",
       " 'NB': GaussianNB()}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## cosine similarity on designed features\n",
    "le = preprocessing.LabelEncoder()\n",
    "df = pd.read_csv(\"trainSet_yoga82.csv\")\n",
    "get_designed_data_results(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3YTPcT37J7n"
   },
   "outputs": [],
   "source": [
    "! gdown --id 18WNBVg98h_JKXCSKaZDELalBXZkvExI3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLyZx_4l5pXP",
    "outputId": "a9035610-eaf8-487e-825d-1f2e48bb235e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17513/17513 [00:58<00:00, 300.48it/s]\n",
      "100%|██████████| 17513/17513 [00:26<00:00, 664.73it/s]\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "LOGISTIC REGRESSION\n",
      "Testing Data:\n",
      "Accuracy: 0.5172413793103449\n",
      "F1 Score: 0.4951959748881769\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 0.5266483934825643\n",
      "F1 Score: 0.5083430772877348\n",
      "##################################################\n",
      "SGD CLASSIFIER\n",
      "Testing Data:\n",
      "Accuracy: 0.31491208038364926\n",
      "F1 Score: 0.2840277804901138\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 0.32183645500228414\n",
      "F1 Score: 0.2909727063599277\n",
      "##################################################\n",
      "GAUSSIAN NAIVE BAYES\n",
      "Testing Data:\n",
      "Accuracy: 0.8833066910253482\n",
      "F1 Score: 0.884005508979424\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 0.8934064260697426\n",
      "F1 Score: 0.8939559353149742\n",
      "##################################################\n",
      "RANDOM FOREST\n",
      "Testing Data:\n",
      "Accuracy: 0.9282941310801552\n",
      "F1 Score: 0.9278770392281098\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 1.0\n",
      "F1 Score: 1.0\n",
      "##################################################\n",
      "XGBOOST\n",
      "Testing Data:\n",
      "Accuracy: 0.9125371089289792\n",
      "F1 Score: 0.9122518950732736\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 0.9969544693162784\n",
      "F1 Score: 0.996954078507356\n",
      "##################################################\n",
      "ADABOOST\n",
      "Testing Data:\n",
      "Accuracy: 0.0701073304407399\n",
      "F1 Score: 0.009492101807305623\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 0.0702756205268768\n",
      "F1 Score: 0.01003501538883573\n",
      "##################################################\n",
      "KNN CLASSIFIER\n",
      "Testing Data:\n",
      "Accuracy: 0.5944279515871204\n",
      "F1 Score: 0.5845152815715018\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 0.6946855489569057\n",
      "F1 Score: 0.6893246534740088\n",
      "##################################################\n",
      "SVM\n",
      "Testing Data:\n",
      "Accuracy: 0.5361954784197306\n",
      "F1 Score: 0.5074250320181612\n",
      "--------------------------------------------------\n",
      "Training Data:\n",
      "Accuracy: 0.5422567382366378\n",
      "F1 Score: 0.5140814513758741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ADA': AdaBoostClassifier(),\n",
       " 'KNN': KNeighborsClassifier(),\n",
       " 'NB': GaussianNB(),\n",
       " 'RF': RandomForestClassifier(),\n",
       " 'SGD': SGDClassifier(),\n",
       " 'SVM': SVC(),\n",
       " 'XGB': XGBClassifier(objective='multi:softprob'),\n",
       " 'logistic': LogisticRegression()}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "df = pd.read_csv(\"trainSet_yoga82.csv\")\n",
    "get_designed_data_results(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQ81olcF5pXQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "df = pd.read_csv(\"trainSet_yoga82.csv\")\n",
    "get_raw_data_results(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extended_Revolved_Triangle_Pose_or_Utthita_Trikonasana_'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([y_pred[0]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHZcvNfP8jh7"
   },
   "source": [
    "2. ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "JnNQAYX95pXQ",
    "outputId": "d1b087e7-6529-485d-eb82-58120fc1e52b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 524.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_pose: ['Extended_Revolved_Triangle_Pose_or_Utthita_Trikonasana_']\n",
      "getting cosine similarity results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1956.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe saved with eval results!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path =  \"/home/hardeekh/Downloads/custom_dataset/self_img/\"#.jpg\"\n",
    "with open(\"custom_feature_model_dict.pkl\",\"rb\") as handle:\n",
    "    model_dict = pickle.load(handle)\n",
    "    \n",
    "for i in range(16,17):\n",
    "    \n",
    "    lol = path+str(i)+\".jpg\"\n",
    "    y_pred=predict_image(lol, model_dict[\"logistic\"], le, True)\n",
    "    \n",
    "    if i==16:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mig1GGPl5pXR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import math\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def predict_image(model, path, le):\n",
    "    \n",
    "    mp_pose = mp.solutions.pose\n",
    "    mp_drawing = mp.solutions.drawing_utils \n",
    "    mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    \n",
    "    with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:\n",
    "        \n",
    "        img = cv2.imread(path)\n",
    "     \n",
    "        results = pose.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "      #add result to trainSet\n",
    "        row = []\n",
    "        for i in range(0,33):\n",
    "            coord = results.pose_landmarks.landmark[i]\n",
    "            row.append(coord.x)\n",
    "            row.append(coord.y)\n",
    "            row.append(coord.z)\n",
    "            row.append(coord.visibility)\n",
    "          #row.append(dict2[foldername])\n",
    "\n",
    "    row = np.array(row).reshape(1,-1)\n",
    "   # print(row)\n",
    "    print((model.predict_proba(row)))\n",
    "    print(le.inverse_transform(model.predict(row)))\n",
    "    \n",
    "#predict_image(model, \"/home/hardeekh/Downloads/custom_dataset/self)img/\", le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kErETIxN5pXR",
    "outputId": "ab2c1abf-389c-41d6-91e1-e0521a72b98a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08 0.   0.05 0.4  0.02 0.2  0.15 0.05 0.01 0.04]]\n",
      "['extended_side_angle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12 0.04 0.03 0.13 0.01 0.3  0.18 0.09 0.02 0.08]]\n",
      "['seated_forward_bend']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6  0.08 0.06 0.   0.13 0.01 0.06 0.   0.03 0.03]]\n",
      "['cobra']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01 0.04 0.   0.06 0.   0.   0.02 0.03 0.   0.84]]\n",
      "['warrior-3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05 0.12 0.01 0.03 0.02 0.38 0.1  0.08 0.01 0.2 ]]\n",
      "['seated_forward_bend']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11 0.01 0.07 0.4  0.01 0.05 0.14 0.12 0.07 0.02]]\n",
      "['extended_side_angle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09 0.29 0.06 0.03 0.   0.01 0.11 0.13 0.27 0.01]]\n",
      "['dancer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28 0.01 0.01 0.07 0.01 0.21 0.34 0.03 0.03 0.01]]\n",
      "['split']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03 0.01 0.19 0.17 0.09 0.24 0.13 0.1  0.03 0.01]]\n",
      "['seated_forward_bend']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54 0.05 0.03 0.05 0.03 0.   0.16 0.01 0.11 0.02]]\n",
      "['cobra']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03 0.04 0.01 0.03 0.   0.   0.05 0.16 0.65 0.03]]\n",
      "['warrior-2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.48 0.05 0.03 0.1  0.03 0.03 0.17 0.02 0.05 0.04]]\n",
      "['cobra']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02 0.01 0.47 0.14 0.07 0.14 0.1  0.04 0.01 0.  ]]\n",
      "['extended_revolved_triangle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01 0.   0.01 0.02 0.   0.36 0.45 0.12 0.01 0.02]]\n",
      "['split']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12 0.02 0.   0.16 0.   0.35 0.14 0.08 0.05 0.08]]\n",
      "['seated_forward_bend']\n",
      "[[0.   0.   0.2  0.61 0.06 0.05 0.04 0.01 0.02 0.01]]\n",
      "['extended_side_angle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/hardeekh/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "path = \"/home/hardeekh/Downloads/custom_dataset/self_img/\"\n",
    "for file in glob.glob(os.path.join(path,\"*\")):\n",
    "    \n",
    "    with open(os.path.join(os.getcwd(),file),\"r\") as f:\n",
    "        \n",
    "        predict_image(model,f.name,le)\n",
    "        \n",
    "        \n",
    "                   # cv2.imread(f.name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_M2DS1_5pXR",
    "outputId": "a8916987-2cfb-4e77-aa10-5201d152cb6c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hardeekh/Downloads/custom_dataset/self_img/9.jpeg\n",
      "[[0.   0.34 0.23 0.33 0.06 0.02 0.01 0.   0.01 0.  ]]\n",
      "['dancer']\n",
      "/home/hardeekh/Downloads/custom_dataset/self_img/12.jpeg\n",
      "[[0.03 0.73 0.12 0.06 0.01 0.04 0.01 0.   0.   0.  ]]\n",
      "['dancer']\n",
      "/home/hardeekh/Downloads/custom_dataset/self_img/14.jpeg\n",
      "[[0.92 0.01 0.06 0.   0.   0.   0.01 0.   0.   0.  ]]\n",
      "['cobra']\n",
      "/home/hardeekh/Downloads/custom_dataset/self_img/7.jpeg\n",
      "[[0.1  0.54 0.34 0.01 0.01 0.   0.   0.   0.   0.  ]]\n",
      "['dancer']\n",
      "/home/hardeekh/Downloads/custom_dataset/self_img/6.jpg\n",
      "[[0.63 0.06 0.1  0.01 0.09 0.02 0.04 0.04 0.   0.01]]\n",
      "['cobra']\n",
      "/home/hardeekh/Downloads/custom_dataset/self_img/5.jpg\n",
      "[[0.01 0.68 0.2  0.07 0.02 0.01 0.01 0.   0.   0.  ]]\n",
      "['dancer']\n",
      "/home/hardeekh/Downloads/custom_dataset/self_img/11.jpeg\n",
      "[[0.15 0.07 0.18 0.23 0.31 0.   0.03 0.03 0.   0.  ]]\n",
      "['half_moon']\n",
      "/home/hardeekh/Downloads/custom_dataset/self_img/13.jpeg\n",
      "[[0.13 0.08 0.12 0.09 0.03 0.46 0.07 0.02 0.   0.  ]]\n",
      "['seated_forward_bend']\n",
      "/home/hardeekh/Downloads/custom_dataset/self_img/8.jpeg\n",
      "[[0.   0.33 0.22 0.33 0.08 0.02 0.02 0.   0.   0.  ]]\n",
      "['dancer']\n",
      "/home/hardeekh/Downloads/custom_dataset/self_img/2.jpg\n",
      "[[0.75 0.01 0.04 0.   0.02 0.11 0.04 0.03 0.   0.  ]]\n",
      "['cobra']\n",
      "/home/hardeekh/Downloads/custom_dataset/self_img/3.jpg\n",
      "[[0.07 0.15 0.21 0.47 0.08 0.01 0.   0.01 0.   0.  ]]\n",
      "['extended_side_angle']\n",
      "/home/hardeekh/Downloads/custom_dataset/self_img/10.jpeg\n",
      "[[0.92 0.   0.   0.02 0.01 0.04 0.01 0.   0.   0.  ]]\n",
      "['cobra']\n",
      "/home/hardeekh/Downloads/custom_dataset/self_img/1.jpg\n",
      "[[0.06 0.08 0.59 0.1  0.17 0.   0.   0.   0.   0.  ]]\n",
      "['extended_revolved_triangle']\n",
      "/home/hardeekh/Downloads/custom_dataset/self_img/15.jpeg\n",
      "[[0.33 0.35 0.15 0.01 0.03 0.11 0.   0.02 0.   0.  ]]\n",
      "['dancer']\n",
      "/home/hardeekh/Downloads/custom_dataset/self_img/4.jpg\n",
      "[[0.04 0.64 0.17 0.06 0.05 0.04 0.   0.   0.   0.  ]]\n",
      "['dancer']\n",
      "/home/hardeekh/Downloads/custom_dataset/self_img/16.jpeg\n",
      "[[0.   0.42 0.58 0.   0.   0.   0.   0.   0.   0.  ]]\n",
      "['extended_revolved_triangle']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "path = \"/home/hardeekh/Downloads/custom_dataset/self_img/\"\n",
    "for file in glob.glob(os.path.join(path,\"*\")):\n",
    "    \n",
    "    with open(os.path.join(os.getcwd(),file),\"r\") as f:\n",
    "        \n",
    "        lol = predict_image(model,f.name,le)\n",
    "        \n",
    "        features = get_custom_features(lol)\n",
    "        #print(lol)\n",
    "        print(f.name)\n",
    "        print((model.predict_proba(features)))\n",
    "        print(le.inverse_transform(model.predict(features)))\n",
    "                   # cv2.imread(f.name)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "almost_done_lesser_features_upd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
